# On-Device_LLMs_Gemma_Jina_Reader
Implementation
To set up and run the MediaPipe LLM Inference task for web applications, follow these steps:

Ensure your browser supports WebGPU, like Chrome on macOS or Windows.
Create a folder named llm_task.
Copy index.html and index.js files into your llm_task folder.
Download the Gemma 2B model from Gemma or convert an external LLM model (Phi-2, Falcon, or StableLM) into the llm_task folder, ensuring it’s compatible with a GPU backend.
In the index.js file, update the modelFileName variable to match your model file’s name.
Run a local server within the llm_task folder using the command python -m http.server 8080 or python -m SimpleHTTPServer 8080 for older Python versions.
Open localhost:8080 in your Chrome browser. The web interface will activate, ready for use in about 10 seconds.
About Me🚀
Hello! I’m Toni Ramchandani 👋. I’m deeply passionate about all things technology! My journey is about exploring the vast and dynamic world of tech, from cutting-edge innovations to practical business solutions. I believe in the power of technology to transform our lives and work. 🌐

Let’s connect at https://www.linkedin.com/in/toni-ramchandani/ and exchange ideas about the latest tech trends and advancements! 🌟
